{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 04-01 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "测试各种权重初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_neural, num_layer):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_neural, num_neural, bias=False) for i in range(num_layer)])\n",
    "        self.num_neural = num_neural\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            # x = torch.tanh(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            print('layer:{}, std:{}'.format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print('output is nan in {} layers'.format(i))\n",
    "                break\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # nn.init.normal_(m.weight.data)\n",
    "                # nn.init.normal_(m.weight.data, std=np.sqrt(1 / self.num_neural))\n",
    "\n",
    "                # tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                # nn.init.xavier_normal_(m.weight.data, gain=tanh_gain)\n",
    "\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "layer:0, std:0.8585270643234253\nlayer:1, std:0.8404558300971985\nlayer:2, std:0.8249557018280029\nlayer:3, std:0.8484933376312256\nlayer:4, std:0.8268516659736633\nlayer:5, std:0.8528571724891663\nlayer:6, std:0.8680320382118225\nlayer:7, std:0.8814204335212708\nlayer:8, std:0.7839422225952148\nlayer:9, std:0.8308531045913696\nlayer:10, std:0.9206832647323608\nlayer:11, std:0.9202840328216553\nlayer:12, std:0.8722168207168579\nlayer:13, std:0.8734568357467651\nlayer:14, std:0.933708906173706\nlayer:15, std:0.8420002460479736\nlayer:16, std:0.8669116497039795\nlayer:17, std:0.8451323509216309\nlayer:18, std:0.8712531328201294\nlayer:19, std:0.9319099187850952\nlayer:20, std:0.7731372117996216\nlayer:21, std:0.7305227518081665\nlayer:22, std:0.7169121503829956\nlayer:23, std:0.6657664775848389\nlayer:24, std:0.608913242816925\nlayer:25, std:0.5542789101600647\nlayer:26, std:0.5616728067398071\nlayer:27, std:0.5216749906539917\nlayer:28, std:0.4437980353832245\nlayer:29, std:0.43753883242607117\nlayer:30, std:0.42527881264686584\nlayer:31, std:0.35999584197998047\nlayer:32, std:0.39501628279685974\nlayer:33, std:0.39576244354248047\nlayer:34, std:0.39303791522979736\nlayer:35, std:0.4273345172405243\nlayer:36, std:0.39641183614730835\nlayer:37, std:0.4135555028915405\nlayer:38, std:0.44271355867385864\nlayer:39, std:0.436187744140625\nlayer:40, std:0.4591417908668518\nlayer:41, std:0.4810537099838257\nlayer:42, std:0.4720474183559418\nlayer:43, std:0.523200511932373\nlayer:44, std:0.5088015794754028\nlayer:45, std:0.5228967666625977\nlayer:46, std:0.5496976375579834\nlayer:47, std:0.5482072234153748\nlayer:48, std:0.5491491556167603\nlayer:49, std:0.5568612217903137\nlayer:50, std:0.5456120371818542\nlayer:51, std:0.5017459988594055\nlayer:52, std:0.5032362341880798\nlayer:53, std:0.5107277631759644\nlayer:54, std:0.5191236138343811\nlayer:55, std:0.569932222366333\nlayer:56, std:0.6115372776985168\nlayer:57, std:0.6824691295623779\nlayer:58, std:0.6913095712661743\nlayer:59, std:0.6979054808616638\nlayer:60, std:0.7124932408332825\nlayer:61, std:0.6556227207183838\nlayer:62, std:0.755199670791626\nlayer:63, std:0.79518061876297\nlayer:64, std:0.7385206818580627\nlayer:65, std:0.7227587699890137\nlayer:66, std:0.6823157668113708\nlayer:67, std:0.642217218875885\nlayer:68, std:0.6704565286636353\nlayer:69, std:0.7117814421653748\nlayer:70, std:0.8180145025253296\nlayer:71, std:0.8276257514953613\nlayer:72, std:0.7281336784362793\nlayer:73, std:0.7339627146720886\nlayer:74, std:0.7505672574043274\nlayer:75, std:0.6599897742271423\nlayer:76, std:0.6529918313026428\nlayer:77, std:0.7289100289344788\nlayer:78, std:0.7606716156005859\nlayer:79, std:0.7494245767593384\nlayer:80, std:0.7580430507659912\nlayer:81, std:0.7493789196014404\nlayer:82, std:0.7309451699256897\nlayer:83, std:0.7032414078712463\nlayer:84, std:0.6426860094070435\nlayer:85, std:0.642347514629364\nlayer:86, std:0.6073390245437622\nlayer:87, std:0.6635287404060364\nlayer:88, std:0.6434864401817322\nlayer:89, std:0.6221253275871277\nlayer:90, std:0.684719979763031\nlayer:91, std:0.6954287886619568\nlayer:92, std:0.7211251258850098\nlayer:93, std:0.6956448554992676\nlayer:94, std:0.7155397534370422\nlayer:95, std:0.7757969498634338\nlayer:96, std:0.7267651557922363\nlayer:97, std:0.683974027633667\nlayer:98, std:0.6022020578384399\nlayer:99, std:0.5744592547416687\ntensor([[0.2919, 0.9336, 0.0000,  ..., 0.9148, 0.0000, 0.0000],\n        [0.1045, 0.5365, 0.0000,  ..., 0.5689, 0.0000, 0.0000],\n        [0.1848, 0.7143, 0.0000,  ..., 0.6667, 0.0000, 0.0000],\n        ...,\n        [0.2370, 0.7904, 0.0000,  ..., 0.6875, 0.0000, 0.0000],\n        [0.1240, 0.5469, 0.0000,  ..., 0.6696, 0.0000, 0.0000],\n        [0.1588, 0.8896, 0.0000,  ..., 0.8193, 0.0000, 0.0000]],\n       grad_fn=<ReluBackward0>)\n"
    }
   ],
   "source": [
    "num_layer = 100\n",
    "num_nerual = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(num_nerual, num_layer)\n",
    "net.initialize_weights()\n",
    "\n",
    "inputs = torch.randn((batch_size, num_nerual))\n",
    "outputs = net(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "计算激活函数的方差变化尺度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}