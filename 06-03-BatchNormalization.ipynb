{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 06-03 Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.3.1'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.Batch Normalization and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "layers:0, std:0.5761674642562866\nlayers:1, std:0.5949079394340515\nlayers:2, std:0.578766942024231\nlayers:3, std:0.5829827189445496\nlayers:4, std:0.5845738649368286\nlayers:5, std:0.5818923115730286\nlayers:6, std:0.5772672891616821\nlayers:7, std:0.5799934267997742\nlayers:8, std:0.5806533694267273\nlayers:9, std:0.5835395455360413\nlayers:10, std:0.5771958827972412\nlayers:11, std:0.5888833403587341\nlayers:12, std:0.5752442479133606\nlayers:13, std:0.585811972618103\nlayers:14, std:0.5752991437911987\nlayers:15, std:0.5831204652786255\nlayers:16, std:0.5782873630523682\nlayers:17, std:0.578441321849823\nlayers:18, std:0.5825368762016296\nlayers:19, std:0.5842101573944092\nlayers:20, std:0.5705402493476868\nlayers:21, std:0.5783528685569763\nlayers:22, std:0.5758209228515625\nlayers:23, std:0.5793800950050354\nlayers:24, std:0.580898106098175\nlayers:25, std:0.5877857208251953\nlayers:26, std:0.5863972306251526\nlayers:27, std:0.5840093493461609\nlayers:28, std:0.5815438032150269\nlayers:29, std:0.5767336487770081\nlayers:30, std:0.5770955681800842\nlayers:31, std:0.5775684714317322\nlayers:32, std:0.5841580629348755\nlayers:33, std:0.5797197222709656\nlayers:34, std:0.5842310786247253\nlayers:35, std:0.5823767781257629\nlayers:36, std:0.5803950428962708\nlayers:37, std:0.5771668553352356\nlayers:38, std:0.5698117017745972\nlayers:39, std:0.5788571238517761\nlayers:40, std:0.5741791129112244\nlayers:41, std:0.5827059745788574\nlayers:42, std:0.5814263820648193\nlayers:43, std:0.5893250703811646\nlayers:44, std:0.5811281800270081\nlayers:45, std:0.5868545770645142\nlayers:46, std:0.5843479037284851\nlayers:47, std:0.5836506485939026\nlayers:48, std:0.5814852118492126\nlayers:49, std:0.5850608348846436\nlayers:50, std:0.5845655202865601\nlayers:51, std:0.572669267654419\nlayers:52, std:0.5690715312957764\nlayers:53, std:0.5777657628059387\nlayers:54, std:0.5806640386581421\nlayers:55, std:0.5800541639328003\nlayers:56, std:0.5849465131759644\nlayers:57, std:0.5779260993003845\nlayers:58, std:0.5796957015991211\nlayers:59, std:0.5824030637741089\nlayers:60, std:0.5798154473304749\nlayers:61, std:0.5760301947593689\nlayers:62, std:0.5760180354118347\nlayers:63, std:0.5754522681236267\nlayers:64, std:0.583263635635376\nlayers:65, std:0.5884244441986084\nlayers:66, std:0.5801348090171814\nlayers:67, std:0.5839254260063171\nlayers:68, std:0.5835193395614624\nlayers:69, std:0.5741776823997498\nlayers:70, std:0.5763088464736938\nlayers:71, std:0.5817015767097473\nlayers:72, std:0.5817670822143555\nlayers:73, std:0.5880773067474365\nlayers:74, std:0.5773541927337646\nlayers:75, std:0.5859130024909973\nlayers:76, std:0.5771793127059937\nlayers:77, std:0.5775134563446045\nlayers:78, std:0.5775967240333557\nlayers:79, std:0.5832148790359497\nlayers:80, std:0.5807939171791077\nlayers:81, std:0.5914139747619629\nlayers:82, std:0.590234100818634\nlayers:83, std:0.5841630101203918\nlayers:84, std:0.5817498564720154\nlayers:85, std:0.585745632648468\nlayers:86, std:0.5794791579246521\nlayers:87, std:0.5777183771133423\nlayers:88, std:0.585939347743988\nlayers:89, std:0.5731821060180664\nlayers:90, std:0.5780108571052551\nlayers:91, std:0.577186107635498\nlayers:92, std:0.5809988379478455\nlayers:93, std:0.5885604023933411\nlayers:94, std:0.5797430276870728\nlayers:95, std:0.5836082100868225\nlayers:96, std:0.5732372999191284\nlayers:97, std:0.5855777263641357\nlayers:98, std:0.5777072906494141\nlayers:99, std:0.5792800784111023\ntensor([[0.0000, 0.0000, 0.0000,  ..., 0.3603, 0.9320, 0.0000],\n        [0.0000, 0.9752, 0.0000,  ..., 0.0000, 2.0840, 0.8566],\n        [0.0000, 0.8580, 0.0754,  ..., 0.5351, 0.1982, 0.0000],\n        ...,\n        [0.0000, 0.0000, 0.2268,  ..., 0.1988, 0.7582, 0.0000],\n        [0.6796, 0.0330, 0.0000,  ..., 0.0000, 0.0000, 2.4377],\n        [0.0000, 1.2569, 0.3563,  ..., 0.0000, 0.0000, 0.9124]],\n       grad_fn=<ReluBackward0>)\n"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_hidden, num_layers=100):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_hidden, num_hidden, bias=False) for i in range(num_layers)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(num_hidden) for i in range(num_layers)])\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (linear, bn) in enumerate(zip(self.linears, self.bns)):\n",
    "            x = linear(x)\n",
    "            x = bn(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            if torch.isnan(x.std()):\n",
    "                print('output is nan in {} layers'.format(i))\n",
    "                break\n",
    "            print('layers:{}, std:{}'.format(i, x.std().item()))\n",
    "        return x\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # nn.init.normal_(m.weight.data, std=1)\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "num_hidden = 256\n",
    "num_layers = 100\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(num_hidden, num_layers)\n",
    "net.initialize()\n",
    "\n",
    "inputs = torch.randn((batch_size, num_hidden))\n",
    "outputs = net(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.Batch Normalization Function"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "input data:\n tensor([[[1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.]],\n\n        [[1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.]],\n\n        [[1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.]]])\n shape: torch.Size([3, 5, 1])\niteration: 0, running mean: tensor([0.3000, 0.6000, 0.9000, 1.2000, 1.5000])\niteration: 0, running var: tensor([0.7000, 0.7000, 0.7000, 0.7000, 0.7000])\niteration: 0, 2nd feature running mean: 0.6\niteration: 0, 2nd feature running var: 0.7\niteration: 1, running mean: tensor([0.5100, 1.0200, 1.5300, 2.0400, 2.5500])\niteration: 1, running var: tensor([0.4900, 0.4900, 0.4900, 0.4900, 0.4900])\niteration: 1, 2nd feature running mean: 1.02\niteration: 1, 2nd feature running var: 0.48999999999999994\n"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "num_features = 5\n",
    "momentum = 0.3\n",
    "\n",
    "feature_shape = (1,)\n",
    "\n",
    "feature_map = torch.ones(feature_shape)\n",
    "feature_maps = torch.stack([feature_map * (i + 1) for i in range(num_features)], dim=0)\n",
    "feature_maps_bs = torch.stack([feature_maps for i in range(batch_size)], dim=0)\n",
    "print('input data:\\n {}\\n shape: {}'.format(feature_maps_bs, feature_maps_bs.shape))\n",
    "\n",
    "bn = nn.BatchNorm1d(num_features=num_features, momentum=momentum)\n",
    "\n",
    "running_mean, running_var = 0, 1\n",
    "\n",
    "for i in range(2):\n",
    "    outputs = bn(feature_maps_bs)\n",
    "    print('iteration: {}, running mean: {}'.format(i, bn.running_mean))\n",
    "    print('iteration: {}, running var: {}'.format(i, bn.running_var))\n",
    "\n",
    "    mean_t, var_t = 2, 0\n",
    "\n",
    "    running_mean = (1 - momentum) * running_mean + momentum * mean_t\n",
    "    running_var = (1 - momentum) * running_var + momentum * var_t\n",
    "\n",
    "    print('iteration: {}, 2nd feature running mean: {}'.format(i, running_mean))\n",
    "    print('iteration: {}, 2nd feature running var: {}'.format(i, running_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "input data:\n tensor([[[[1., 1.],\n          [1., 1.]],\n\n         [[2., 2.],\n          [2., 2.]],\n\n         [[3., 3.],\n          [3., 3.]],\n\n         [[4., 4.],\n          [4., 4.]],\n\n         [[5., 5.],\n          [5., 5.]],\n\n         [[6., 6.],\n          [6., 6.]]],\n\n\n        [[[1., 1.],\n          [1., 1.]],\n\n         [[2., 2.],\n          [2., 2.]],\n\n         [[3., 3.],\n          [3., 3.]],\n\n         [[4., 4.],\n          [4., 4.]],\n\n         [[5., 5.],\n          [5., 5.]],\n\n         [[6., 6.],\n          [6., 6.]]],\n\n\n        [[[1., 1.],\n          [1., 1.]],\n\n         [[2., 2.],\n          [2., 2.]],\n\n         [[3., 3.],\n          [3., 3.]],\n\n         [[4., 4.],\n          [4., 4.]],\n\n         [[5., 5.],\n          [5., 5.]],\n\n         [[6., 6.],\n          [6., 6.]]]])\n shape: torch.Size([3, 6, 2, 2])\niteration: 0, running_mean.shape: torch.Size([6])\niteration: 0, running_var.shape: torch.Size([6])\niteration: 0, weight.shape: torch.Size([6])\niteration: 0, bias.shape: torch.Size([6])\niteration: 1, running_mean.shape: torch.Size([6])\niteration: 1, running_var.shape: torch.Size([6])\niteration: 1, weight.shape: torch.Size([6])\niteration: 1, bias.shape: torch.Size([6])\n"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "num_features = 6\n",
    "momentum = 0.3\n",
    "\n",
    "feature_shape = (2, 2)\n",
    "\n",
    "feature_map = torch.ones(feature_shape)\n",
    "feature_maps = torch.stack([feature_map * (i + 1) for i in range(num_features)], dim=0)\n",
    "feature_maps_bs = torch.stack([feature_maps for i in range(batch_size)], dim=0)\n",
    "print('input data:\\n {}\\n shape: {}'.format(feature_maps_bs, feature_maps_bs.shape))\n",
    "\n",
    "bn = nn.BatchNorm2d(num_features=num_features, momentum=momentum)\n",
    "\n",
    "running_mean, running_var = 0, 1\n",
    "\n",
    "for i in range(2):\n",
    "    outputs = bn(feature_maps_bs)\n",
    "    print('iteration: {}, running_mean.shape: {}'.format(i, bn.running_mean.shape))\n",
    "    print('iteration: {}, running_var.shape: {}'.format(i, bn.running_var.shape))\n",
    "\n",
    "    print('iteration: {}, weight.shape: {}'.format(i, bn.weight.shape))\n",
    "    print('iteration: {}, bias.shape: {}'.format(i, bn.bias.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "input data:\n tensor([[[[[1., 1., 1.],\n           [1., 1., 1.]],\n\n          [[1., 1., 1.],\n           [1., 1., 1.]]],\n\n\n         [[[2., 2., 2.],\n           [2., 2., 2.]],\n\n          [[2., 2., 2.],\n           [2., 2., 2.]]],\n\n\n         [[[3., 3., 3.],\n           [3., 3., 3.]],\n\n          [[3., 3., 3.],\n           [3., 3., 3.]]],\n\n\n         [[[4., 4., 4.],\n           [4., 4., 4.]],\n\n          [[4., 4., 4.],\n           [4., 4., 4.]]]],\n\n\n\n        [[[[1., 1., 1.],\n           [1., 1., 1.]],\n\n          [[1., 1., 1.],\n           [1., 1., 1.]]],\n\n\n         [[[2., 2., 2.],\n           [2., 2., 2.]],\n\n          [[2., 2., 2.],\n           [2., 2., 2.]]],\n\n\n         [[[3., 3., 3.],\n           [3., 3., 3.]],\n\n          [[3., 3., 3.],\n           [3., 3., 3.]]],\n\n\n         [[[4., 4., 4.],\n           [4., 4., 4.]],\n\n          [[4., 4., 4.],\n           [4., 4., 4.]]]],\n\n\n\n        [[[[1., 1., 1.],\n           [1., 1., 1.]],\n\n          [[1., 1., 1.],\n           [1., 1., 1.]]],\n\n\n         [[[2., 2., 2.],\n           [2., 2., 2.]],\n\n          [[2., 2., 2.],\n           [2., 2., 2.]]],\n\n\n         [[[3., 3., 3.],\n           [3., 3., 3.]],\n\n          [[3., 3., 3.],\n           [3., 3., 3.]]],\n\n\n         [[[4., 4., 4.],\n           [4., 4., 4.]],\n\n          [[4., 4., 4.],\n           [4., 4., 4.]]]]])\n shape: torch.Size([3, 4, 2, 2, 3])\niteration: 0, running_mean.shape: torch.Size([4])\niteration: 0, running_var.shape: torch.Size([4])\niteration: 0, weight.shape: torch.Size([4])\niteration: 0, bias.shape: torch.Size([4])\niteration: 1, running_mean.shape: torch.Size([4])\niteration: 1, running_var.shape: torch.Size([4])\niteration: 1, weight.shape: torch.Size([4])\niteration: 1, bias.shape: torch.Size([4])\n"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "num_features = 4\n",
    "momentum = 0.3\n",
    "\n",
    "feature_shape = (2, 2, 3)\n",
    "\n",
    "feature_map = torch.ones(feature_shape)\n",
    "feature_maps = torch.stack([feature_map * (i + 1) for i in range(num_features)], dim=0)\n",
    "feature_maps_bs = torch.stack([feature_maps for i in range(batch_size)], dim=0)\n",
    "print('input data:\\n {}\\n shape: {}'.format(feature_maps_bs, feature_maps_bs.shape))\n",
    "\n",
    "bn = nn.BatchNorm3d(num_features=num_features, momentum=momentum)\n",
    "\n",
    "running_mean, running_var = 0, 1\n",
    "\n",
    "for i in range(2):\n",
    "    outputs = bn(feature_maps_bs)\n",
    "    print('iteration: {}, running_mean.shape: {}'.format(i, bn.running_mean.shape))\n",
    "    print('iteration: {}, running_var.shape: {}'.format(i, bn.running_var.shape))\n",
    "\n",
    "    print('iteration: {}, weight.shape: {}'.format(i, bn.weight.shape))\n",
    "    print('iteration: {}, bias.shape: {}'.format(i, bn.bias.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}